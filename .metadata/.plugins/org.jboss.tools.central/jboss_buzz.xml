<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Retail data framework - Common architectural elements</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/xXFiSsZFv0A/retail-data-framework-common-architectural-elements.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/zejVwlU1WVc/retail-data-framework-common-architectural-elements.html</id><updated>2021-06-01T05:00:00Z</updated><content type="html">Part 2 - Common architectural elements  In our  from this series we introduced a use case around the data framework for retail stores. The process was laid out how we've approached the use case and how portfolio solutions are the base for researching a generic architectural blueprint.  The only thing left to cover was the order in which you'll be led through the blueprint details. This article starts the real journey at the very top, with a generic architecture from which we'll discuss the common architectural elements one by one. This will start our journey into the logical elements that make up the real-time stock control architecture blueprint. BLUEPRINTS REVIEW As mentioned before, the architectural details covered here are base on real solutions using open source technologies. The example scenario presented here is a generic common blueprint that was uncovered researching those solutions. It's our intent to provide a blueprint that provides guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architectural blueprint, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. FROM SPECIFIC TO GENERIC Before diving in to the common elements, it might be nice to understand that this is not a catch all for every possible supply chain integration solution. It's a collection of identified elements that we've uncovered in multiple customer implementations. These elements presented here are then the generic common architectural elements that we've identified and collected in to the generic architectural blueprint.  It's our intent to provide a blueprint for guidance and not deep technical details. You're smart enough to figure out wiring integration points in your own architectures. You're capable of slotting in the technologies and components you've committed to in the past where applicable.  It's our job here to describe the architectural blueprint generic components and outline a few specific cases with visual diagrams so that you're able to make the right decisions from the start of your own projects. Another challenge has been how to visually represent the architectural blueprint. There are many ways to represent each element, but we've chosen some icons, text and colours that we hope are going to make it all easy to absorb. Now let's take a quick tour of the generic architecture and outline the common elements uncovered in my research. DATA INTEGRATION PLATFORM The logical view splits this solution space into several identifiable platforms where the retail data framework is managed. It takes all three of these platforms to ensure that the data is collected, integrated with the various blueprints external to the framework, processed, validated, stored, data science is applied for insights, and exposed back out to the entire retail organisation. The first we'll look at is the data integration platform where the main action takes place with the retail data framework. Here there are integration microservices and data integration microservices to provide integration with the core platform, data science platform, and storage services.  Another important set of elements found in this platform are messaging and event processing. Both are essential elements to ensure microservice communications and message transformation within the architecture. To help with data performance, availability, and management there are data caching microservices and data virtualisation microservices.  Next up, process automation is used to capture processes within the retail organisation, manage the processing and validation of data in a structured traceable manner. The business automation microservices capture all of these processes and ensure proper monitoring of the compliance and regulatory rules for the entire retail organisation.  Finally, ensuring that the fronting web applications have good visual data representations requires that all microservices are only accessed by authenticated and authorised parties. This is taken care of through the use of an API management element.  You can sense that this data integration platform contains the elements focusing on microservice deployments which lend themselves to a cloud-native development process using containers and container platforms.  CORE PLATFORM A core platform focusing on security and compliance requires the hosting of retail organisation wide tooling. These are not specifically called out and can be any number of core services or systems hosted within the retail organisation or outside in the form of Software as a Service (SaaS) solutions. This platform hosts a set of four elements that each support the organisation, starting with compliance and regulatory tooling. This is not the rules mentioned in the previous section, but the tooling backing the development, deployment, and maintenance of the compliance and regulatory rules.  There should be some form of auditing tooling and governance tooling used to ensure data and the services used to support the retail data framework are properly monitored in their application. Finally, the authentication and authorisation tooling is the central system used to plug in organisational wide access to the right parties within the retail data framework. DATA SCIENCE PLATFORM Any retail organisation working in their markets has a vast interest in the behaviour patterns of their customers. At the very least they are using the most basic data science elements, and in advanced cases, they are leveraging all forms of data science to advance their market positions. In the data science platform we find the more classical business intelligence tooling, often an externally hosted system noted here with a private cloud icon. Providing views and cuts of the mass data collected in retail organisations is the fundamental function of this element.  The advanced use of not just analytics on their data, but applying more sophisticated technologies like data science (AI / ML) allows retail organisations to gain advantageous insights into customers, trends, products, sales, and other retail activities that raw data analysis can't provide. Finally, there is data visualisation tooling that provides clear visibility to the consumers of the data and analysis generated from the other elements on this platform. STORAGE SERVICES The storage services uncovered in this solution space was a fairly diverse and more high level than the usually noted storage elements found in our architecture blueprints. As these storage needs are data focused and organisation wide solutions, you find all the major technologies in the data world applied here, such as data lakes, data warehousing, data hubs, and data marts. WHAT'S NEXT This was just a short overview of the common generic elements that make up our architecture blueprint for the retail data framework use case.  An overview of this series on retail data framework portfolio architecture blueprint: 1. 2. 3. Example data architecture Catch up on any past articles you missed by following any published links above. Next in this series, taking a look at an example data framework architecture for this blueprint. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/xXFiSsZFv0A" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/zejVwlU1WVc/retail-data-framework-common-architectural-elements.html</feedburner:origLink></entry><entry><title type="html">Infinispan (Red Hat Data Grid) featured in Red Hat Developers</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/lF7ai7DIYvI/infinispan-redhat-summit-quarkus" /><author><name>Katia Aresti</name></author><id>https://infinispan.org/blog/2021/05/31/infinispan-redhat-summit-quarkus</id><updated>2021-05-31T12:00:00Z</updated><content type="html">Dear Infinispan Community, The Infinispan team is pleased to share an article published over on the Red Hat Developer blog. Part one of a two-part series of articles, this blog post focuses on how Data Grid, which is built on Infinispan, was used to create a leaderboard for an online Battleship game that featured at this year’s Red Hat Summit Keynote. You can read the blog post Technologies featured in our keynote demo: * Quarkus and Infinispan Client Extension * Additional Quarkus extensions: RestEasy, Websockets, Scheduler * Infinispan Query * Infinispan Cross-Site Replication * Infinispan Kubernetes Operator Enjoy your reading!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/lF7ai7DIYvI" height="1" width="1" alt=""/&gt;</content><dc:creator>Katia Aresti</dc:creator><feedburner:origLink>https://infinispan.org/blog/2021/05/31/infinispan-redhat-summit-quarkus</feedburner:origLink></entry><entry><title>Learn Quarkus faster with quick starts in the Developer Sandbox for Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/FSoQUzD7WPk/learn-quarkus-faster-quick-starts-developer-sandbox-red-hat-openshift" /><author><name>Daniel Oh</name></author><id>9e2c023a-1daf-4ba7-b00d-1bf2469de902</id><updated>2021-05-31T07:00:00Z</updated><published>2021-05-31T07:00:00Z</published><summary type="html">&lt;p&gt;Java developers are usually required to take many actions before we can begin developing and deploying cloud-native &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. First, we have to configure everything from the integrated development environment (IDE) to build tools such as &lt;a href="https://maven.apache.org"&gt;Maven&lt;/a&gt; or &lt;a href="https://gradle.org"&gt;Gradle&lt;/a&gt;. We also need to configure the command-line tools used for containerization and generating the Kubernetes manifest. If we don’t want to spin up a Kubernetes cluster locally, we also must connect to a remote Kubernetes cluster for continuous testing and deployment.&lt;/p&gt; &lt;p&gt;Developers should spend less time on configuration and more time accelerating the inner-loop development cycle of building, testing, and deploying our applications. Ideally, we should be able to continuously develop applications in a pre-configured Kubernetes environment.&lt;/p&gt; &lt;p&gt;This article is a guide to configuring Java applications using &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt; quick starts in the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;. As you'll see, using quick starts in the developer sandbox lets you focus on the inner loop of development, without needing to configure the Kubernetes cluster or development tools.&lt;/p&gt; &lt;p&gt;Developers using the developer sandbox have access to a shared, multi-tenant &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift 4&lt;/a&gt; cluster with a set of pre-installed developer tools such as a web-based IDE and &lt;a href="https://developers.redhat.com/products/codeready-workspaces/overview"&gt;Red Hat CodeReady Workspaces&lt;/a&gt;. You can get started in less than five minutes with a free Red Hat developer account. To learn more about the developer sandbox, click &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Step 1: Launch your developer sandbox&lt;/h2&gt; &lt;p&gt;Assuming you have set up and signed into your Red Hat developer account, you can start your OpenShift environment in the developer sandbox. Go to the &lt;a href="https://developers.redhat.com/developer-sandbox/get-started"&gt;Get started in the Sandbox&lt;/a&gt; page, then click on &lt;strong&gt;Start using your Sandbox&lt;/strong&gt;, as shown in Figure 1.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="The 'Get started in the Sandbox' option is selected." data-entity-type="file" data-entity-uuid="b962a4e4-1ead-4fb6-a35c-4dbab2a3f534" height="380" src="https://developers.redhat.com/sites/default/files/inline-images/Screen%20Shot%202021-05-24%20at%205.21.07%20PM.png" width="593" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: Start using your developer sandbox.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: You must choose &lt;strong&gt;DevSandbox&lt;/strong&gt; to log into the cluster.&lt;/p&gt; &lt;h2&gt;Step 2: Explore Quarkus quick starts&lt;/h2&gt; &lt;p&gt;Once you log in, you will arrive at the OpenShift topology view. Click &lt;strong&gt;View all Quick Starts&lt;/strong&gt; then enter a search for "Quarkus." You will see the three quick starts shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Enter a search for 'Quarkus' to view the three available Quarkus quick starts." data-entity-type="file" data-entity-uuid="765f40f4-ee54-4177-a1b0-8453ba53c653" height="197" src="https://developers.redhat.com/sites/default/files/inline-images/Screen%20Shot%202021-05-24%20at%205.22.49%20PM.png" width="586" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2: Quarkus quick starts in the OpenShift topology view.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Step 3: Open your first Quarkus quick start&lt;/h2&gt; &lt;p&gt;Select the "Get started with Quarkus using S2I" quick start shown in Figure 2. This 10-minute quick start takes you through the six tasks to create and deploy a Quarkus application on OpenShift using a source-to-image (S2I) approach:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Create the Quarkus application.&lt;/li&gt; &lt;li&gt;View the build status.&lt;/li&gt; &lt;li&gt;View the associated Git repository.&lt;/li&gt; &lt;li&gt;View the pod status.&lt;/li&gt; &lt;li&gt;Change the deployment icon to Quarkus.&lt;/li&gt; &lt;li&gt;Run the Quarkus application.&lt;/li&gt; &lt;/ol&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The Quarkus project is automatically created for you in the sandbox environment, so you can skip the first task and complete the quick start faster.&lt;/p&gt; &lt;h2&gt;Step 4: Run the 'Get started with Quarkus using S2I' quick start&lt;/h2&gt; &lt;p&gt;When you are ready, click &lt;strong&gt;Start tour&lt;/strong&gt;, as shown in Figure 3.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="The six tasks are shown, along with the option to start the tour." data-entity-type="file" data-entity-uuid="66c5f6e0-cc7b-499d-ae69-7ab23292c9db" height="424" src="https://developers.redhat.com/sites/default/files/inline-images/Screen%20Shot%202021-05-24%20at%205.24.16%20PM_1.png" width="456" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 3: Start the tour.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The tour guides you through the step-by-step instructions to complete the tasks in this quick start. You can skip the first task because it has already been done automatically for you, as shown in Figure 4.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="The project has already been created automatically." data-entity-type="file" data-entity-uuid="b67d70e4-fa40-4bc5-8b76-1eb007e04ea0" height="347" src="https://developers.redhat.com/sites/default/files/inline-images/Screen%20Shot%202021-05-24%20at%205.25.30%20PM.png" width="588" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 4: The Quarkus application has already been created.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;When you complete each step, click &lt;strong&gt;Next&lt;/strong&gt; to verify your work. If you have accomplished the task without any issues, click &lt;strong&gt;Yes&lt;/strong&gt;, then click &lt;strong&gt;Next&lt;/strong&gt; again.&lt;/p&gt; &lt;p&gt;Most tasks in this quick start are self-explanatory, but a couple of them are worth exploring.&lt;/p&gt; &lt;h3&gt;Task 3: View the associated Git repository&lt;/h3&gt; &lt;p&gt;The developer sandbox lets developers change application code directly using CodeReady Workspaces instead of navigating to the Git repository from a local IDE. This makes pre-configuring the application easier, as I mentioned earlier. You also can run the "getting started" application in Quarkus development mode, which lets you use Quarkus's live coding feature for continuous development.&lt;/p&gt; &lt;p&gt;When you click the &lt;strong&gt;CodeReady Workspaces&lt;/strong&gt; icon in the bottom-right quadrant of the &lt;strong&gt;quarkus-quickstarts&lt;/strong&gt; deployment, it brings you to CodeReady Workspaces, as shown in Figure 5.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="CodeReady Workspaces opens in the console." data-entity-type="file" data-entity-uuid="0b229b5c-9059-418f-8844-5af1a0bda0eb" height="317" src="https://developers.redhat.com/sites/default/files/inline-images/Screen%20Shot%202021-05-24%20at%205.27.16%20PM.png" width="569" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 5: View your code in CodeReady Workspaces.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Task 6: Run the Quarkus application&lt;/h3&gt; &lt;p&gt;In Task 6, you can access the Quarkus application's REST API. Click the external link icon to open the URL and run the application in a new browser tab, as shown in Figure 6.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Run the Quarkus application from the REST API." data-entity-type="file" data-entity-uuid="aefc7785-9e09-48bb-bffd-c263a105b841" height="311" src="https://developers.redhat.com/sites/default/files/inline-images/Screen%20Shot%202021-05-24%20at%205.28.12%20PM.png" width="570" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 6: Open the Quarkus application URL.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Step 5: Finish the quick start&lt;/h2&gt; &lt;p&gt;When you have completed all six tasks you will see the green checkmarks shown in Figure 7.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Each task has a green checkmark beside it." data-entity-type="file" data-entity-uuid="e9e9ae91-9c5e-497f-b6d5-2ca8d8c7b1f5" height="462" src="https://developers.redhat.com/sites/default/files/inline-images/Screen%20Shot%202021-05-24%20at%205.28.56%20PM_0.png" width="527" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 7: The green checkmarks indicate that all tasks have been completed.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Great job! Now, you can tour another quick start or repeat this one as a learning practice.&lt;/p&gt; &lt;h2&gt;Watch the Quarkus quick starts video demonstration&lt;/h2&gt; &lt;p&gt;If you want further instruction, this video demonstration guides you through two of the three available Quarkus quick starts: "Get started with Quarkus using S2I" and "Get started with Quarkus using a Helm chart."&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, you've learned how much faster you can get started with Quarkus application development and deployment using Quarkus quick starts in the Developer Sandbox for Red Hat OpenShift. The sandbox is free for all business application developers who are interested in cloud-native microservices development using Quarkus. The sandbox lets you use a modern web-based IDE for development and deploy your cloud-native microservices applications to a Red Hat OpenShift cluster seamlessly. Use the self-service learning portal &lt;a href="https://developers.redhat.com/courses/quarkus"&gt;here&lt;/a&gt; to learn more about Java application development with Quarkus.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/05/31/learn-quarkus-faster-quick-starts-developer-sandbox-red-hat-openshift" title="Learn Quarkus faster with quick starts in the Developer Sandbox for Red Hat OpenShift"&gt;Learn Quarkus faster with quick starts in the Developer Sandbox for Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/FSoQUzD7WPk" height="1" width="1" alt=""/&gt;</summary><dc:creator>Daniel Oh</dc:creator><dc:date>2021-05-31T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/05/31/learn-quarkus-faster-quick-starts-developer-sandbox-red-hat-openshift</feedburner:origLink></entry><entry><title type="html">This Week in JBoss - 31 May 2021</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ApC_h-WHvGM/weekly-2021-05-31.html" /><category term="quarkus" /><category term="wildfly" /><category term="keycloak" /><category term="kogito" /><category term="infinispan" /><category term="camel" /><category term="jgroups" /><category term="vert.x" /><author><name>Don Naro</name><uri>https://www.jboss.org/people/don-naro</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2021-05-31.html</id><updated>2021-05-31T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, wildfly, keycloak, kogito, infinispan, camel, jgroups, vert.x"&gt; &lt;h1&gt;This Week in JBoss - 31 May 2021&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hello! Welcome to another edition of the JBoss Editorial that brings you news and updates from our community.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_hello_again"&gt;Hello again&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;To our community and all our readers,&lt;/p&gt; &lt;p&gt;I’d like to start this edition with a sincere and frank apology on behalf of the editorial team for the posts we missed in the last month.&lt;/p&gt; &lt;p&gt;There’s been a lot of awesome content that our community has shared and multiple project releases packed with useful new features and clever enhancements. We’re long overdue in highlighting and celebrating all the great work that JBoss teams are doing, not to mention all the brilliant work of our evangelists and other contributors.&lt;/p&gt; &lt;p&gt;It’s been a busy past few weeks and we’ve got a lot of great articles and releases to catch up on, so let’s go.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Let’s start things off with congrats to all the teams on their hard work!&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://belaban.blogspot.com/2021/05/jgroups-517-released.html/"&gt;JGroups 5.1.7&lt;/a&gt; is released. Congrats, Bela!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-1-13-6-final-released/"&gt;Quarkus 1.13.6.Final&lt;/a&gt; has shipped, which is the latest in a recent series of updates to version 1.13.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-0-0-cr2-released/"&gt;Quarkus 2 CR2&lt;/a&gt; is out!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-4-1-CR2-released/"&gt;Vert.x 4.1.0.CR2&lt;/a&gt; is here, right on the heels of the beta and CR1 releases.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://infinispan.org/blog/2021/05/07/infinispan-12-1-2-final"&gt;Infinispan 12.1.2 Final&lt;/a&gt; is available for download so go grab it.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2021/05/keycloak-1301-released"&gt;Keycloak 13.0.1&lt;/a&gt; is out!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2021/05/kogito-tooling-0-10-0-released.html"&gt;Kogito Tooling 0.10.0&lt;/a&gt; has launched!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://bytemanblog.blogspot.com/2021/05/byteman-4015-has-been-released.html]"&gt;Byteman 4.0.15&lt;/a&gt; is now available!&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_quarkus_2_ama"&gt;Quarkus 2 AMA&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;With the recent release of Quarkus CR2, I’m sure we’re all anticipating Quarkus 2.0 GA. As part of that release, Max and the rest of the Quarkus team are taking questions that they will answer on an episode of Quarkus Insights. Use the &lt;code&gt;#quarkusinsights&lt;/code&gt; tag to submit a question via social media and tune in to &lt;a href="https://www.youtube.com/watch?v=ETTMBWEBfLY"&gt;Quarkus Insights #51: Q &amp;#38; A - Part II&lt;/a&gt; on June 2 to hear your questions answered.&lt;/p&gt; &lt;p&gt;Follow the social media links to post your question to the Quarkus team in their post, &lt;a href="https://quarkus.io/blog/quarkus-insights-qanda2/"&gt;About to release Quarkus 2 - ask us anything!&lt;/a&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_from_the_community"&gt;From the community&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Jeff Mesnil has authored a very helpful and detailed look at building and deploying WildFly applications on OpenShift using Helm Charts in his post, &lt;a href="https://www.wildfly.org/news/2021/05/05/helm-charts-for-wildfly/"&gt;Helm Chart for WildFly&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Over on the Infinispan blog, Ryan Emerson has shared some details about the CLI compiled to a native image in &lt;a href="https://infinispan.org/blog/2021/05/21/infinispan-cli-image"&gt;Infinispan Native CLI&lt;/a&gt;, which is well worth a read. You should also try downloading the latest Infinispan 12 server version and taking it for a spin with the native CLI!&lt;/p&gt; &lt;p&gt;Another recent one from the Infinispan team comes from Katia Aresti who, along with Ryan Emerson, explains how they &lt;a href="https://developers.redhat.com/articles/2021/05/28/building-real-time-leaderboard-red-hat-data-grid-and-quarkus-hybrid-kubernetes"&gt;built a real-time leaderboard using Data Grid and several Quarkus extensions&lt;/a&gt; to add some magic to this year’s Red Hat Summit Keynote.&lt;/p&gt; &lt;p&gt;Jacopo Rota on the Kogito blog explains how to &lt;a href="https://blog.kie.org/2021/05/getting-started-with-trustyai-in-only-15-minutes.html"&gt;how to deploy a Kogito service together with the TrustyAI infrastructure on an OpenShift cluster in only 15 minutes&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;Have you been wanting to find out more about Shenandoah GC? Well, you should dive right in and check out Roman Kennke’s informative post, &lt;a href="https://developers.redhat.com/articles/2021/05/20/shenandoah-garbage-collection-openjdk-16-concurrent-reference-processing"&gt;Shenandoah garbage collection in OpenJDK 16: Concurrent reference processing&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Bilgin Ibryam has recently posted &lt;a href="http://www.ofbizian.com/2021/05/data-gateways-of-future.html"&gt;Data Gateways in the Cloud Native Era&lt;/a&gt; that examines how data gateway components support different use cases and offer a solution for hybrid workloads spread across multiple cloud providers.&lt;/p&gt; &lt;p&gt;Last but certainly not least is Claus Ibsen’s webinar, &lt;a href="http://www.davsclaus.com/2021/05/webinar-integrate-systems-in-age-of.html"&gt;Integrate systems in the age of Quarkus and Camel&lt;/a&gt;, that explores how the trio of Camel Quarkus, Camel K, and Kamelets simplify the work to manage and bind systems together.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_evangelists_corner"&gt;Evangelist’s corner&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Eric Schabell never disappoints and his previous series on architectural elements in a real-time stock control solution for retail was no exception. Eric rounds that series off nicely with &lt;a href="https://www.schabell.org/2021/05/real-time-stock-control-example-stock-control-architecture.html"&gt;Real-time stock control - Example stock control architecture&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Not one to rest for long, Eric Schabell launches a new series that tackles how to create a framework for accessing retail data from customers, stock, stores, and staff across multiple internal teams. I’m sure it’s going to be a brilliant series so go have a look for yourself and find out more in his post, &lt;a href="https://www.schabell.org/2021/05/retail-data-framework-architectural-introduction.html"&gt;Retail data framework - An architectural introduction&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I also really enjoyed reading through Christina and Eric’s &lt;a href="http://wei-meilin.blogspot.com/2021/05/tooling-guide-for-getting-started-with.html"&gt;Tooling guide for Getting Started with Apache Camel in 2021&lt;/a&gt;. It’s a well put together beginner’s guide to tools that can help with Camel applications.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_developers_on_film"&gt;Developers on film&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Get your popcorn ready and sit back to watch some videos from our community. Here are my top picks for this week’s editorial:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/ILl85LLj93w"&gt;Quarkus Insights #49: Why I use Quarkus for Cloud Native Apps&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/7JPm1BFcrrk"&gt;What is Serverless with Java?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/jBDmX85IjLM"&gt;No YAML! Kubernetes done the easy way&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/szza3DZlKzA"&gt;Quarkus DevServices&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/don-naro.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Don Naro&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ApC_h-WHvGM" height="1" width="1" alt=""/&gt;</content><dc:creator>Don Naro</dc:creator><feedburner:origLink>https://www.jboss.org/posts/weekly-2021-05-31.html</feedburner:origLink></entry><entry><title type="html">Data Gateways in the Cloud Native Era</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/r9Na-7W7j-Y/data-gateways-of-future.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2021/05/data-gateways-of-future.html</id><updated>2021-05-29T11:52:00Z</updated><content type="html">These days, there is a lot of excitement around 12-factor apps, microservices, and service mesh, but not so much around cloud-native data. The number of conference talks, blog posts, best practices, and purpose-built tools around cloud-native data access is relatively low. One of the main reasons for this is because most data access technologies are architectured and created in a stack that favors static environments rather than the dynamic nature of cloud environments and Kubernetes. In this article, we will explore the different categories of data gateways, from more monolithic to ones designed for the cloud and Kubernetes. We will see what are the technical challenges introduced by the Microservices architecture and how data gateways can complement API gateways to address these challenges in the Kubernetes era. APPLICATION ARCHITECTURE EVOLUTIONS Let’s start with what has been changing in the way we manage code and the data in the past decade or so. I still remember the time when I started my IT career by creating frontends with Servlets, JSP, and JSFs. In the backend, EJBs, SOAP, server-side session management, was the state of art technologies and techniques. But things changed rather quickly with the introduction of REST and popularization of Javascript. REST helped us decouple frontends from backends through a uniform interface and resource-oriented requests. It popularized stateless services and enabled response caching, by moving all client session state to clients, and so forth. This new architecture was the answer to the huge scalability demands of modern businesses. A similar change happened with the backend services through the Microservices movement. Decoupling from the frontend was not enough, and the monolithic backend had to be decoupled into bounded context enabling independent fast-paced releases. These are examples of how architectures, tools, and techniques evolved pressured by the business needs for fast software delivery of planet-scale applications. That takes us to the data layer. One of the existential motivations for microservices is having independent data sources per service. If you have microservices touching the same data, that sooner or later introduces coupling and limits independent scalability or releasing. It is not only an independent database but also a heterogeneous one, so every microservice is free to use the database type that fits its needs. Application architecture evolution brings new challenges While decoupling frontend from backend and splitting monoliths into microservices gave the desired flexibility, it created challenges not-present before. Service discovery and load balancing, network-level resilience, and observability turned into major areas of technology innovation addressed in the years that followed. Similarly, creating a database per microservice, having the freedom and technology choice of different datastores is a challenge. That shows itself more and more recently with the explosion of data and the demand for accessing data not only by the services but other real-time reporting and AI/ML needs. THE RISE OF API GATEWAYS With the increasing adoption of Microservices, it became apparent that operating such an architecture is hard. While having every microservice independent sounds great, it requires tools and practices that we didn’t need and didn’t have before. This gave rise to more advanced release strategies such as blue/green deployments, canary releases, dark launches. Then that gave rise to fault injection and automatic recovery testing. And finally, that gave rise to advanced network telemetry and tracing. All of these created a whole new layer that sits between the frontend and the backend. This layer is occupied primarily with API management gateways, service discovery, and service mesh technologies, but also with tracing components, application load balancers, and all kinds of traffic management and monitoring proxies. This even includes projects such as Knative with activation and scaling-to-zero features driven by the networking activity. With time, it became apparent that creating microservices at a fast pace, operating microservices at scale requires tooling we didn’t need before. Something that was fully handled by a single load balancer had to be replaced with a new advanced management layer. A new technology layer, a new set of practices and techniques, and a new group of users responsible were born. THE CASE FOR DATA GATEWAYS Microservices influence the data layer in two dimensions. First, it demands an independent database per microservice. From a practical implementation point of view, this can be from an independent database instance to independent schemas and logical groupings of tables. The main rule here is, only one microservice owns and touches a dataset. And all data is accessed through the APIs or Events of the owning microservice. The second way a microservices architecture influenced the data layer is through datastore proliferation. Similarly, enabling microservices to be written in different languages, this architecture allows the freedom for every microservices-based system to have a  persistence layer. With this freedom, one microservice can use a relational database, another one can use a document database, and the third microservice one uses an in-memory key-value store. While microservices allow you all that freedom, again it comes at a cost. It turns out operating a large number of datastore comes at a cost that existing tooling and practices were not prepared for. In the modern digital world, storing data in a reliable form is not enough. Data is useful when it turns into insights and for that, it has to be accessible in a controlled form by many. AI/ML experts, data scientists, business analysts, all want to dig into the data, but the application-focused microservices and their data access patterns are not  for these data-hungry demands. API and Data gateways offering similar capabilities at different layers This is where data gateways can help you. A data gateway is like an API gateway, but it understands and acts on the physical data layer rather than the networking layer. Here are a few areas where data gateways differ from API gateways. ABSTRACTION An API gateway can hide implementation endpoints and help upgrade and rollback services without affecting service consumers. Similarly, a data gateway can help abstract a physical data source, its specifics, and help alter, migrate, decommission, without affecting data consumers. SECURITY An API manager secures resource endpoints based on HTTP methods. A service mesh secures based on network connections. But none of them can understand and secure the data and its shape that is passing through them. A data gateway, on the other hand, understands the different data sources and the data model and acts on them. It can apply RBAC per data row and column, filter, obfuscate, and sanitize the individual data elements whenever necessary. This is a more fine-grained security model than networking or API level security of API gateways. SCALING API gateways can do service discovery, load-balancing, and assist the scaling of services through an orchestrator such as Kubernetes. But they cannot scale data. Data can scale only through replication and caching. Some data stores can do replication in cloud-native environments but not all. Purpose-built tools, such as , can perform change data capture from the transaction logs of data stores and enable data replication for scaling and other use cases. A data gateway, on the other hand, can speed-up access to all kinds of data sources by caching data and providing materialized views. It can understand the queries, optimize them based on the capabilities of the data source, and produce the most performant execution plan. The combination of materialized views and the stream nature of change data capture would be the ultimate data scaling technique, but there are no known cloud-native implementations of this yet. FEDERATION In API management, response composition is a common technique for aggregating data from multiple different systems. In the data space, the same technique is referred to as heterogeneous data federation. Heterogeneity is the degree of differentiation in various data sources such as network protocols, query languages, query capabilities, data models, error handling, transaction semantics, etc. A data gateway can accommodate all of these differences as a seamless, transparent data-federation layer. SCHEMA-FIRST API gateways allow contract-first service and client development with specifications such as OpenAPI. Data gateways allow schema-first data consumption based on the SQL standard. A SQL schema for data modeling is the OpenAPI equivalent of APIs. MANY SHADES OF DATA GATEWAYS In this article, I use the terms API and data gateways loosely to refer to a set of capabilities. There are many types of API gateways such as API managers, load balancers, service mesh, service registry, etc. It is similar to data gateways, where they range from huge monolithic data virtualization platforms that want to do everything, to data federation libraries, from purpose-built cloud services to end-user query tools. Let’s explore the different types of data gateways and see which fit the definition of “a cloud-native data gateway.” When I say a cloud-native data gateway, I mean a containerized first-class Kubernetes citizen. I mean a gateway that is open source, using open standards; a component that can be deployed on hybrid/multi-cloud infrastructures, work with different data sources, data formats, and applicable for many use cases. CLASSIC DATA VIRTUALIZATION PLATFORMS In the very first category of data gateways, are the traditional data virtualization platforms such as  and . While these are the most feature-laden data platforms, they tend to do too much and want to be everything from API management, to metadata management, data cataloging, environment management, deployment, configuration management, and whatnot. From an architectural point of view, they are very much like the old ESBs, but for the data layer. You may manage to put them into a container, but it is hard to put them into the cloud-native citizen category. DATABASES WITH DATA FEDERATION CAPABILITIES Another emerging trend is the fact that databases, in addition to storing data, are also starting to act as data federation gateways and allowing access to external data. For example, PostgreSQL  the ANSI SQL/MED specification for a standardized way of handling access to remote objects from SQL databases. That means remote data stores, such as SQL, NoSQL, File, LDAP, Web, Big Data, can all be accessed as if they were tables in the same PostgreSQL database. SQL/MED stands for Management of External Data, and it is also implemented by MariaDB  engine, , Teiid project discussed below, and a few . Starting in SQL Server 2019, you can now query external data sources without moving or copying the data. The  engine of SQL Server instance to process Transact-SQL queries to access external data in SQL Server, Oracle, Teradata, and MongoDB. GRAPHQL DATA BRIDGES Compared to the traditional data virtualization, this is a new category of data gateways focused around the fast web-based data access. The common thing around , , , is that they focus on GraphQL data access by offering a lightweight abstraction on top of a few data sources. This is a fast-growing category specialized for enabling rapid web-based development of data-driven applications rather than BI/AI/ML use cases. OPEN-SOURCE DATA GATEWAYS  is a schema-free SQL query engine for NoSQL databases and file systems. It offers JDBC and ODBC access to business users, analysts, and data scientists on top of data sources that don’t support such APIs. Again, having uniform SQL based access to disparate data sources is the driver. While Drill is highly scalable, it relies on Hadoop or Apache Zookeeper’s kind of infrastructure which shows its age.  is a mature data federation engine sponsored by Red Hat. It uses the SQL/MED specification for defining the virtual data models and relies on the Kubernetes Operator model for the building, deployment, and management of its runtime. Once deployed, the runtime can scale as any other stateless cloud-native workload on Kubernetes and integrate with other cloud-native projects. For example, it can use  for single sign-on and data roles,  for distributed caching needs, export metrics and register with Prometheus for monitoring, Jaeger for tracing, and even with for API management. But ultimately, Teiid runs as a single Spring Boot application acting as a data proxy and integrating with other best-of-breed services on Openshift rather than trying to reinvent everything from scratch. Architectural overview of Teiid data gateway On the client-side, Teiid offers standard SQL over JDBC/ODBC and Odata APIs. Business users, analysts, and data scientists can use standard BI/analytics tools such as Tableau, MicroStrategy, Spotfire, etc. to interact with Teiid. Developers can leverage the REST API or JDBC for custom built microservices and serverless workloads. In either case, for data consumers, Teiid appears as a standard PostgreSQL database accessed over its JDBC or ODBC protocols but offering additional abstractions and decoupling from the physical data sources.  is another popular open-source project started by Facebook. It is a distributed SQL query engine targeting big data use cases through its coordinator-worker architecture. The Coordinator is responsible for parsing statements, planning queries, managing workers, fetching results from the workers, and returning the final results to the client. The worker is responsible for executing tasks and processing data.  Some time ago, the founders split from PrestoDB and created a fork called (formerly PrestoSQL). Today, PrestoDB is part of The Linux Foundation, and Trino part of Trino Software Foundation. Both distributions of Presto are among the most active and powerful open-source data gateway projects in this space. To learn more about this technology, is a good book I found. CLOUD-HOSTED DATA GATEWAYS SERVICES With a move to the cloud infrastructure, the need for data gateways doesn’t go away but increases instead. Here are a few cloud-based data gateway services:  is ANSI SQL based interactive query service for analyzing data tightly integrated with Amazon S3. It is based on PrestoDB and supports additional data sources and federation capabilities too. Another similar service by Amazon is . It is focused around the same functionality, i.e. querying S3 objects using SQL. The main difference is that Redshift Spectrum requires a Redshift cluster, whereas Athena is a serverless offering that doesn’t require any servers.  is a similar service but from Google. These tools require minimal to no setup, they can access on-premise or cloud-hosted data and process huge datasets. But they couple you with a single cloud provider as they cannot be deployed on multiple clouds or on-premise. They are ideal for interactive querying rather than acting as hybrid data frontend for other services and tools to use. SECURE TUNNELING DATA-PROXIES With cloud-hosted data gateways comes the need for accessing on-premise data. Data has gravity and also might be affected by regulatory requirements preventing it from moving to the cloud. It may also be a conscious decision to keep the most valuable asset (your data) from cloud-coupling. All of these cases require cloud access to on-premise data. And cloud providers make it easy to reach your data. Azure’s  is such a proxy allowing access to on-premise data stores from Azure Service Bus. In the opposite scenario, accessing cloud-hosted data stores from on-premise clients can be challenging too. Google’s  provides secure access to Cloud SQL instances without having to whitelist IP addresses or configure SSL. Red Hat-sponsored open-source project  takes the more generic approach to address these challenges. Skupper solves Kubernetes multi-cluster communication challenges through a layer 7 virtual network that offers advanced routing and secure connectivity capabilities. Rather than embedding Skupper into the business service runtime, it runs as a standalone instance per Kubernetes namespace and acts as a shared sidecar capable of secure tunneling for data access or other general service-to-service communication. It is a generic secure-connectivity proxy applicable for many use cases in the hybrid cloud world. CONNECTION POOLS FOR SERVERLESS WORKLOADS Serverless takes software decomposition a step further from microservices. Rather than services splitting by bounded context, serverless is based on the function model where every operation is short-lived and performs a single operation. These granular software constructs are extremely scalable and flexible but come at a cost that previously wasn’t present. It turns out rapid scaling of functions is a challenge for connection-oriented data sources such as relational databases and message brokers. As a result cloud providers offer transparent data proxies as a service to manage connection pools effectively.  is such a service that sits between your application and your relational database to efficiently manage connections to the database and improve scalability. CONCLUSION Modern cloud-native architectures combined with the microservices principles enable the creation of highly scalable and independent applications. The large choice of data storage engines, cloud-hosted services, protocols, and data formats, gives the ultimate flexibility for delivering software at a fast pace. But all of that comes at a cost that becomes increasingly visible with the need for uniform real-time data access from emerging user groups with different needs. Keeping microservices data only for the microservice itself creates challenges that have no good technological and architectural answers yet. Data gateways, combined with cloud-native technologies offer features similar to API gateways but for the data layer that can help address these new challenges. The data gateways vary in specialization, but they tend to consolidate on providing uniform SQL-based access, enhanced security with data roles, caching, and abstraction over physical data stores. Data has gravity, requires granular access control, is hard to scale, and difficult to move on/off/between cloud-native infrastructures. Having a data gateway component as part of the cloud-native tooling arsenal, which is hybrid and works on multiple cloud providers, supports different use cases is becoming a necessity. This article was originally published on InfoQ .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/r9Na-7W7j-Y" height="1" width="1" alt=""/&gt;</content><dc:creator>Unknown</dc:creator><feedburner:origLink>http://www.ofbizian.com/2021/05/data-gateways-of-future.html</feedburner:origLink></entry><entry><title type="html">Webinar: Integrate systems in the age of Quarkus and Camel</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/g-Owy-ABX_4/webinar-integrate-systems-in-age-of.html" /><author><name>Claus Ibsen</name></author><id>http://feedproxy.google.com/~r/ApacheCamel/~3/BkZSRxVBBG4/webinar-integrate-systems-in-age-of.html</id><updated>2021-05-29T09:15:00Z</updated><content type="html">A few days ago I presented a webinar where he covered all the latest innovations with Apache Camel with focus on Camel Quarkus, Camel K, and Kamelets. This trio is a powerful combination that takes Camel to another level, which allows non developers and IT professionals, to manage and bind systems together without any Camel knowledge. Kamelets being the Apache Camel solution for an app store experience with integration software. The webinar is and the .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/g-Owy-ABX_4" height="1" width="1" alt=""/&gt;</content><dc:creator>Claus Ibsen</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/ApacheCamel/~3/BkZSRxVBBG4/webinar-integrate-systems-in-age-of.html</feedburner:origLink></entry><entry><title>How to install Kubeflow 1.2 on Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Lvgqj4tFZM4/how-install-kubeflow-12-red-hat-openshift" /><author><name>David Marcus</name></author><id>78521b1a-011c-4ff6-b07a-3c5e1142b49e</id><updated>2021-05-28T07:00:00Z</updated><published>2021-05-28T07:00:00Z</published><summary type="html">&lt;p&gt;As artificial intelligence (AI) adoption increases across industries, particularly through machine learning (ML), the job of integrating the often disparate tools, libraries, packages, and dependencies also increases in complexity. This makes development and operations (&lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt;) a daunting task that both organizations and open source communities are actively working on. To quote the authors of &lt;a href="https://web.kaust.edu.sa/Faculty/MarcoCanini/classes/CS290E/F19/papers/tech-debt.pdf"&gt;Hidden Technical Debt in Machine Learning Systems&lt;/a&gt;, "developing and deploying ML systems is relatively fast and cheap, but maintaining them over time is difficult and expensive."&lt;/p&gt; &lt;p&gt;If you are in the throes of tackling DevOps for &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;AI/ML&lt;/a&gt; (MLOps), two open source projects worth your attention are the upstream &lt;a href="https://www.kubeflow.org/"&gt;Kubeflow&lt;/a&gt; and the downstream &lt;a href="https://opendatahub.io/"&gt;Open Data Hub&lt;/a&gt; (ODH). The goal of these projects is to provide machine learning toolkits that handle the complex parts of orchestration that traditional software DevOps does not.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: For more about MLOps, see, &lt;a href="https://www.openshift.com/blog/dotscience-on-openshift"&gt;Dotscience on OpenShift: Enabling DevOps for MLOps&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;As the name indicates, Kubeflow is based on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. In this article, we'll show it running on &lt;a href="https://developers.redhat.com/products/openshift"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; and include an &lt;a href="https://developers.redhat.com/topics/service-mesh"&gt;Istio service mesh&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Objective&lt;/h2&gt; &lt;p&gt;Use this article as a startup procedure to install a default Kubeflow toolkit on an OpenShift Container Platform instance to explore the tools and capabilities. Figure 1 shows the Kubeflow dashboard running on OpenShift Container Platform, providing access to a suite of machine learning tools that span the system life cycle.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/04/kubeflow-ui.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/04/kubeflow-ui.png?itok=G5FBZdI9" width="600" height="444" alt="Screenshot of the kubeflow central dashboard" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The Kubeflow central dashboard. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The Kubeflow central dashboard.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The latest release of Kubeflow at the time of this writing incorporates changes to the file structure for distribution-specific platforms, such as OpenShift. If you are interested in the details, you can read the source &lt;a href="https://github.com/kubeflow/manifests/pull/1739"&gt;pull request&lt;/a&gt; that explains the reason for the change.&lt;/p&gt; &lt;h3&gt;Overview of major steps&lt;/h3&gt; &lt;p&gt;The following list summarizes the steps needed to get Kubeflow running on OpenShift Container Platform:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Install the Open Data Hub Operator.&lt;/li&gt; &lt;li&gt;Create the Kubeflow project.&lt;/li&gt; &lt;li&gt;Install Kubeflow.&lt;/li&gt; &lt;li&gt;Monitor the installation.&lt;/li&gt; &lt;li&gt;Access the Kubeflow user interface (UI).&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Requirements&lt;/h3&gt; &lt;p&gt;To use Kubeflow as shown in this article, please note the following requirements:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;You &lt;em&gt;must&lt;/em&gt; have an OpenShift Container Platform cluster 4.2+ installed with cluster admin privileges.&lt;/li&gt; &lt;li&gt;You &lt;em&gt;should not&lt;/em&gt; have an &lt;a href="https://istio.io/latest/docs/ops/deployment/deployment-models/#multiple-meshes"&gt;existing Istio service mesh&lt;/a&gt;, because it will lead to name collisions.&lt;/li&gt; &lt;li&gt;You &lt;em&gt;should not&lt;/em&gt;have an existing project named &lt;code&gt;istio-system&lt;/code&gt; as &lt;a href="https://www.kubeflow.org/docs/external-add-ons/istio/istio-in-kubeflow/"&gt;Kubeflow deploys Istio along with configurations&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;You &lt;em&gt;must not&lt;/em&gt; have remaining mutating webhooks or validating webhooks from prior tests.&lt;/li&gt; &lt;li&gt; &lt;p&gt;You &lt;em&gt;must not&lt;/em&gt; deploy Kubeflow in a project or namespace other than &lt;code&gt;kubeflow&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Running on an OpenShift cluster&lt;/h2&gt; &lt;p&gt;Here are some options for getting access to an OpenShift cluster to run through the procedure in this article. Getting a cluster running is beyond the scope of the tutorial, but the resources in this section offer a starting point.&lt;/p&gt; &lt;h3&gt;On your local machine cluster (recommended)&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/codeready-containers/overview"&gt;Red Hat CodeReady Containers&lt;/a&gt; is designed to run on a local computer to simplify setup and testing. The product emulates the cloud development environment with all of the tools needed to develop container-based applications.&lt;/p&gt; &lt;h3&gt;On a 60-minute temporary cluster (only for learning)&lt;/h3&gt; &lt;p&gt;&lt;a href="https://www.katacoda.com/openshift/courses/playgrounds/"&gt;Katacoda&lt;/a&gt; offers an OpenShift cluster as a playground that can be used to perform this installation, as long as you complete the task in an hour or less. It can be done.&lt;/p&gt; &lt;h3&gt;More options&lt;/h3&gt; &lt;p&gt;See the &lt;a href="https://www.openshift.com/try"&gt;OpenShift trial page&lt;/a&gt; for other options.&lt;/p&gt; &lt;h2&gt;Installing the Open Data Hub Operator&lt;/h2&gt; &lt;p&gt;Kubeflow should be installed on OpenShift using the Open Data Hub Operator from the &lt;a href="https://catalog.redhat.com/software/operators/explore"&gt;OpenShift Operators catalog&lt;/a&gt;. The upstream Kubeflow Operator from &lt;a href="http://operatorhub.io"&gt;OperatorHub.io&lt;/a&gt; will not run successfully on OpenShift because it is intended for a general-purpose Kubernetes cluster.&lt;/p&gt; &lt;p&gt;As an administrator from the OpenShift web console, do the following:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Go to &lt;strong&gt;Operators&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Go to &lt;strong&gt;OperatorHub&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Search for "Open Data Hub."&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Open Data Hub Operator&lt;/strong&gt; button.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Continue&lt;/strong&gt; button.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Install&lt;/strong&gt; button.&lt;/li&gt; &lt;li&gt;Accept the default installation strategy, which uses the following settings: &lt;ul&gt;&lt;li&gt;Update Channel: beta&lt;/li&gt; &lt;li&gt;Installation mode: All namespaces on the cluster (default)&lt;/li&gt; &lt;li&gt;Installed Namespace: &lt;code&gt;openshift-operators&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Approval strategy: Automatic&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Install&lt;/strong&gt; button.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 2 illustrates the Open Data Hub Operator selection from the OpenShift OperatorHub.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/04/odh-install.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/04/odh-install.png?itok=piT5UOJ3" width="600" height="307" alt="Screenshot of Open Data Hub Operator install from the Red Hat OpenShift OperatorHub" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Open Data Hub Operator install. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Open Data Hub Operator installation.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Creating the Kubeflow project&lt;/h2&gt; &lt;p&gt;Kubeflow must be installed in a namespace called &lt;code&gt;kubeflow&lt;/code&gt;. A request for an alternative namespace is an &lt;a href="https://github.com/kubeflow/kubeflow/issues/5647"&gt;open issue&lt;/a&gt; at the time of this writing.&lt;/p&gt; &lt;p&gt;As an administrator from the OpenShift web console, do the following:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Go to &lt;strong&gt;Home&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Go to &lt;strong&gt;Projects&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create Project&lt;/strong&gt; button.&lt;/li&gt; &lt;li&gt;Set the following values: &lt;ul&gt;&lt;li&gt;Name: &lt;code&gt;kubeflow&lt;/code&gt; (cannot be altered)&lt;/li&gt; &lt;li&gt;Display Name: &lt;code&gt;kubeflow&lt;/code&gt; (unlike the previous name, you can choose another value here)&lt;/li&gt; &lt;li&gt;Description: Kubeflow ML toolkit (you can choose another value)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Change to the &lt;code&gt;kubeflow&lt;/code&gt; project.&lt;/li&gt; &lt;li&gt;Go to &lt;strong&gt;Operators—&gt;Installed Operators&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Wait for the Operator to display "Succeeded" in the Status field.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 3 displays the expected result when the operator is completely installed.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/04/odh-succeeded.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/04/odh-succeeded.png?itok=Hovsw1gE" width="600" height="191" alt="Screenshot of Open Data Hub Operator with a succeeded status" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: ODH Operator successful installation. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: A succesful installation of the Open Data Hub Operator.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Installing Kubeflow&lt;/h2&gt; &lt;p&gt;By default, the Open Data Hub Operator includes a manifest that lets you try out different components for MLOps. Because the toolset used in this article is different from the one in the default manifest, you should paste in a different manifest.&lt;/p&gt; &lt;p&gt;As an administrator from the OpenShift web console, do the following:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Click the &lt;strong&gt;Open Data Hub Operator&lt;/strong&gt; button.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Open Data Hub&lt;/strong&gt; link under &lt;strong&gt;Provided APIs&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create KfDef&lt;/strong&gt; button.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;YAML View&lt;/strong&gt; radio button.&lt;/li&gt; &lt;li&gt;Delete all the YAML code.&lt;/li&gt; &lt;li&gt;Copy and paste in all the YAML code from &lt;a href="https://raw.githubusercontent.com/kubeflow/manifests/master/distributions/kfdef/kfctl_openshift.v1.2.0.yaml"&gt;kfctl_openshift.v1.2.0.yaml&lt;/a&gt;. &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: For reference, the HTML version can be found on &lt;a href="https://github.com/kubeflow/manifests/tree/master/distributions/kfdef"&gt;Kubeflow GitHub manifests&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create&lt;/strong&gt; button.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 4 shows the Provided APIs selection.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/04/odh-api.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/04/odh-api.png?itok=V9y8Mcio" width="600" height="190" alt="Screenshot of the provided API for selection to create a KfDef and edit YAML" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Open Data Hub Provide API. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Open Data Hub Provided APIs.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Figure 5 shows the YAML code you will replace.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/04/kfctl-yaml.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/04/kfctl-yaml.png?itok=nEXuGo9_" width="600" height="444" alt="Screenshot of the YAML View to replace existing YAML with provided YAML for Kubeflow" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: ODH Provided API KfDef YAML View. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Open Data Hub Provided API KfDef YAML View.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Monitoring the installation&lt;/h2&gt; &lt;p&gt;In the background, the Open Data Hub Operator performs the commands a system administrator would execute on the command line to install Kubeflow, such as &lt;code&gt;kfctl build -f...&lt;/code&gt; and &lt;code&gt;kfctl apply -f...&lt;/code&gt; The web console doesn't show when the installation is complete, so this section shows a few ways to monitor the installation. If all the pods are running without errors, the installation is complete.&lt;/p&gt; &lt;h3&gt;Monitoring from the administrator perspective&lt;/h3&gt; &lt;p&gt;Streaming events are a great way to get a sense of what major activity is occurring after an action such as a deployment. To view the events:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Go to &lt;strong&gt;Home&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Section the project: either &lt;code&gt;kubeflow&lt;/code&gt; to see just events for Kubeflow, or "All projects" to see the multiple projects being updated during installation.&lt;/li&gt; &lt;li&gt;Go to &lt;strong&gt;Events&lt;/strong&gt; to monitor the deployment events stream.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 6 shows the events streaming during an installation in the &lt;code&gt;kubeflow&lt;/code&gt; project.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/04/events.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/04/events.png?itok=oi84DHiL" width="600" height="289" alt="Screenshot of the event stream during a new kubeflow installation" title="Project kubeflow event stream" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Event stream during installation &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: Event stream during installation.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Workload status and alerts are a quick way to understand how progress is going. To view the workloads:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Under &lt;strong&gt;Home&lt;/strong&gt;, click &lt;strong&gt;Projects&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Click the &lt;code&gt;kubeflow&lt;/code&gt; project link.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Workloads&lt;/strong&gt; menu item in the body of the screen to review pods.&lt;/li&gt; &lt;li&gt;Investigate workloads that don't self-correct (give them time to auto-correct).&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 7 shows workloads from the project overview page. Workloads in the project are also viewable from the vertical menu.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/04/overview.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/04/overview.png?itok=OE9glwyg" width="600" height="444" alt="Screenshot of the workloads from the project overview page" title="Kubeflow Workload Overview" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Kubeflow project Workloads overview &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: Overview of the Kubeflow project workloads.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;A project called &lt;code&gt;cert-manager&lt;/code&gt; gets created during installation. Its events and pods provide good insight. To view these events or pods:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Select &lt;strong&gt;Project: cert-manager&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Under &lt;strong&gt;Home&lt;/strong&gt;, click &lt;strong&gt;Events&lt;/strong&gt; to review events. Under &lt;strong&gt;Workloads&lt;/strong&gt;, click &lt;strong&gt;Pods&lt;/strong&gt; to review pods.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 8 shows the pods for &lt;code&gt;cert-manager&lt;/code&gt;.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/04/cert-manager.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/04/cert-manager.png?itok=8DCGiju1" width="600" height="213" alt="Screenshot of the pods in the cert-manager project that gets created" title="Cert-manager project pods" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: Project cert-manager pods status &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 8: Status of pods in the cert-manager pods status.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Another important project, &lt;code&gt;istio-system&lt;/code&gt;, is created during installation. This project hosts the Istio service mesh that handles all the networking between the services. To view the project:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Select &lt;strong&gt;Project: istio-system&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Under &lt;strong&gt;Home&lt;/strong&gt;, click &lt;strong&gt;Events&lt;/strong&gt; to review events. Under &lt;strong&gt;Workloads&lt;/strong&gt;, click &lt;strong&gt;Pods&lt;/strong&gt; to review pods. Under &lt;strong&gt;Networking&lt;/strong&gt;, click &lt;strong&gt;Routes&lt;/strong&gt; to access the URL to the Kubeflow central dashboard.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 9 shows the routes in the project.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/04/istio-route.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/04/istio-route.png?itok=PFfmkJTw" width="600" height="323" alt="Screenshot of the istio ingress gateway route to access kubeflow interface" title="Project istio-system routes" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: Istio-system route for istio-ingressgateway &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 9: istio-system route for the istio-ingress gateway.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Monitoring from the developer perspective&lt;/h3&gt; &lt;p&gt;In addition to the administrator perspective, a developer perspective abstracts infrastructure features out of view to leave an uncluttered developer experience. To see this perspective:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Go to the &lt;strong&gt;developer&lt;/strong&gt; perspective.&lt;/li&gt; &lt;li&gt;Select &lt;strong&gt;Project: kubeflow&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Go to &lt;strong&gt;Topology&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 10 shows the results.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/04/kfctl-topology.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/04/kfctl-topology.png?itok=fLwITOUL" width="600" height="426" alt="Screenshot of the app topology created in the kubeflow project" title="Developer Perspective Topology View" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 9: Developer Perspective kubeflow project topology &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 10: Kubeflow project topology in the developer perspective.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If there are no errors across the projects and the Kubeflow UI launches, the installation has succeeded.&lt;/p&gt; &lt;h2&gt;Accessing the Kubeflow UI&lt;/h2&gt; &lt;p&gt;This section offers two ways to access the Kubeflow central dashboard from the web console. For reference, a command-line query would look like:&lt;/p&gt; &lt;pre&gt; # oc get routes -n istio-system istio-ingressgateway -o jsonpath='http://{.spec.host}/' &lt;/pre&gt; &lt;h3&gt;Going to the dashboard from the administrator perspective&lt;/h3&gt; &lt;p&gt;From the administrator perspective, do the following:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Select &lt;strong&gt;Project: istio-system&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Go to &lt;strong&gt;Networking&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Go to &lt;strong&gt;Routes&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Click the location URL &lt;code&gt;http://istio-ingressgateway...&lt;/code&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 11 shows how to find the location URL.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/04/kubeflow-dashboard.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/04/kubeflow-dashboard.png?itok=xGlofgx1" width="600" height="224" alt="Screenshot of the kubeflow dashboard route in the istio-system project" title="Route to Kubeflow Dashboard" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 10: Route to Kubeflow Dashboard in the istio-system project &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 11: Route to the Kubeflow dashboard in the istio-system project.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Going to the dashboard from the developer perspective&lt;/h3&gt; &lt;p&gt;From the developer perspective, do the following:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Select &lt;strong&gt;Project: istio-system&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Go to &lt;strong&gt;Topology&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Search for "istio-ingressgateway."&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Open URL&lt;/strong&gt; arrow icon, or click the &lt;code&gt;istio-ingressgateway&lt;/code&gt; pod and the URL under &lt;strong&gt;Resources—&gt;Routes&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 12 shows the location of the URL.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/04/kubeflow-route.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/04/kubeflow-route.png?itok=8JfpGtYM" width="600" height="416" alt="Screenshot of the app topology of the istio-system project to access Kubeflow dashboard" title="Kubeflow Topology Route" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 11: Developer Perspective route to Kubeflow from istio-system project &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 12: Developer perspective route to Kubeflow from istio-system project.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Viewing the Kubeflow central dashboard&lt;/h3&gt; &lt;p&gt;Once you complete the registration process and create a namespace, you will see a dashboard like the one in Figure 13.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/04/kubeflow-ui.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/04/kubeflow-ui.png?itok=G5FBZdI9" width="600" height="444" alt="Screenshot of the kubeflow central dashboard" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The Kubeflow central dashboard. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 13: The Kubeflow central dashboard.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Uninstalling Kubeflow&lt;/h2&gt; &lt;p&gt;No proper installation procedure is truly complete without an uninstallation procedure.&lt;/p&gt; &lt;p&gt;As an administrator from the OpenShift web console, do the following:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Select &lt;strong&gt;Project kubeflow&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Open Data Hub&lt;/strong&gt; Operator.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Open Data Hub&lt;/strong&gt; link under &lt;strong&gt;Provided APIs&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Kebab&lt;/strong&gt; button (the one with three vertical dots) for your &lt;code&gt;kubeflow&lt;/code&gt; instance.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Delete KfDef&lt;/strong&gt; to begin the delete process for your &lt;code&gt;kubeflow&lt;/code&gt; instance.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;The procedure in this article illustrates a best practice you can follow to install Kubeflow on Red Hat OpenShift using the Open Data Hub Operator. The manifest file used provides an example toolkit from the Kubeflow project that you can fork, modify, and update to fit your production MLOps needs. Furthermore, the &lt;a href="https://developers.redhat.com/topics/kubernetes/operators"&gt;Operator framework&lt;/a&gt; simplifies installation, operations, and maintenance as the community &lt;a href="https://opendatahub.io/docs/roadmap/future.html"&gt;continues to publish enhancements&lt;/a&gt; to both the operator and machine learning tooling in conjunction with the overall &lt;a href="https://www.openshift.com/learn/topics/ai-ml"&gt;benefits of AI/ML on Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/05/28/how-install-kubeflow-12-red-hat-openshift" title="How to install Kubeflow 1.2 on Red Hat OpenShift"&gt;How to install Kubeflow 1.2 on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Lvgqj4tFZM4" height="1" width="1" alt=""/&gt;</summary><dc:creator>David Marcus</dc:creator><dc:date>2021-05-28T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/05/28/how-install-kubeflow-12-red-hat-openshift</feedburner:origLink></entry><entry><title>How to update to newer Red Hat OpenShift 4 releases</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Q17YrPMCT1I/how-update-newer-red-hat-openshift-4-releases" /><author><name>Fernando Lozano</name></author><id>9ecbbc93-71a1-49f1-b629-956f00987bbb</id><updated>2021-05-27T07:00:00Z</updated><published>2021-05-27T07:00:00Z</published><summary type="html">&lt;p&gt;This article demonstrates two common scenarios for updating &lt;a href="https://developers.redhat.com/products/openshift"&gt;Red Hat OpenShift 4&lt;/a&gt;: to a newer z-stream release and to a newer minor release. I include plenty of screenshots of actual updates from 4.5.4 to 4.5.17 and then to 4.6.4, so you know what to expect when you make these updates yourself.&lt;/p&gt; &lt;p&gt;OpenShift 4 makes the update process easy and provides a number of safety features to minimize the risk of a failed outcome. Still, updating OpenShift clusters can be scary the first time. This article shows how two perform typical update scenarios, step-by-step:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Updating between two z-stream releases, from 4.y.z to 4.y.z+d. Note that you can skip z-stream releases during an update.&lt;/li&gt; &lt;li&gt;Updating between two minor releases, from 4.y to 4.y+1. Note that you cannot skip a minor release during an update.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;To use or not to use the web console&lt;/h2&gt; &lt;p&gt;The update process, including web console pages and quirks, is mostly the same for earlier minor releases, for example a z-stream update of OpenShift 4.4. OpenShift 4.6 includes significant improvements to its web console that solve most if not all of the gotchas that I demonstrate in this article.&lt;/p&gt; &lt;p&gt;You can perform the entire cluster update process from the command-line interface (CLI) and automate it using Ansible playbooks, shell scripts, or whatever you like. If you need instructions about how to perform a cluster update using the CLI, please refer to the &lt;a href="https://docs.openshift.com/container-platform/4.5/updating/updating-cluster-cli.html"&gt;Red Hat OpenShift Container Platform product documentation&lt;/a&gt; and this &lt;a href="https://access.redhat.com/solutions/4606811"&gt;article from the OpenShift knowledgebase&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you have a test cluster that you were considering updating for some time and did not, for fear of the outcome, you can use the cluster now to follow the instructions from this article.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Do not try to update a &lt;a href="https://developers.redhat.com/products/codeready-containers"&gt;Red Hat CodeReady Containers&lt;/a&gt; (CRC) single-node cluster. CodeReady Containers disables some of the cluster operators required for a successful update in order to reduce its hardware requirements, and the OpenShift update process was not designed to work with a single supervisor node, anyway. Later releases of &lt;a href="https://developers.redhat.com/courses/openshift/getting-started"&gt;OpenShift Container Platform&lt;/a&gt; will support single-node OpenShift (SNO) clusters, which follow a different design and installation process from CodeReady Containers. SNO clusters will support updates when they become generally available.&lt;/p&gt; &lt;p&gt;Because the set of available updates and paths changes from time to time, you might not be able to follow the instructions in this article exactly as written. You might start from a different release, target a different final release, and have to pass through different intermediate releases. I hope to provide you with sufficient information to extrapolate to your specific scenario.&lt;/p&gt; &lt;h2&gt;What is the current version of my OpenShift cluster?&lt;/h2&gt; &lt;p&gt;Open your OpenShift web console, log in with cluster administrator rights, and choose &lt;strong&gt;Administration—&gt;Cluster Settings&lt;/strong&gt;. You should see a page that displays your cluster’s current update channel and OpenShift release. Mine is using the somewhat old 4.5.4 release, as shown in Figure 1. I have not updated my test cluster since it was first installed a while ago.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-1-version-4.5.4-2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-1-version-4.5.4-2.png?itok=SzbKg03Q" width="600" height="234" title="blog-1-version-4.5.4" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Viewing the current cluster settings.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If I click on the pen icon beside my current update channel, which is stable-4.5, the web console allows me to choose only from other channels of the same OpenShift release (see Figure 2). That means that my current release cannot be immediately updated to the next minor release—at least not using the web console.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-2-no-4.5-channel.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-2-no-4.5-channel.png?itok=o8Dt20bt" width="600" height="338" title="blog-2-no-4.5-channel" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Available channels for updates.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;For now, do not change the update channel.&lt;/p&gt; &lt;h2&gt;Why don't I see the channel for the next minor OpenShift release?&lt;/h2&gt; &lt;p&gt;It might be possible to update directly to an OpenShift 4.6.x release from the CLI, but up to OpenShift 4.5, the web console comes with a hardcoded list of candidate update channels. The initial z-stream releases hard-code just their own update channels. Later z-stream releases add channels of the next minor release.&lt;/p&gt; &lt;p&gt;OpenShift 4.6 changes the offerings so that the list of available update channels becomes dynamic and shows channels from new releases as they become available. That means that an early 4.6.z release might become updatable to 4.7 without passing through intermediate 4.6.z releases, using the web console.&lt;/p&gt; &lt;p&gt;As I am still on 4.5 and I wish to follow the easy user interface (UI) provided by the web console, I have to first update to a newer 4.5.z release. Shame on me for leaving my cluster for so long without bug fixes and security updates! That is about to be corrected.&lt;/p&gt; &lt;p&gt;If I were in a hurry to update to OpenShift 4.6, or maybe to a very recent 4.5.x release, for example to get the fix for a critical bug that is affecting my production users, I might have to switch to a fast channel. Updates to OpenShift 4 are made available first in the fast channel. Only after a number of customers successfully run a given release without major incidents is an update added to the stable channel.&lt;/p&gt; &lt;p&gt;OpenShift update channels and upgrade paths contain many more nuances than I explain in this article. For deeper information about how Red Hat manages OpenShift update channels and releases, refer to &lt;a href="https://www.openshift.com/blog/the-ultimate-guide-to-openshift-release-and-upgrade-process-for-cluster-administrators"&gt;The Ultimate Guide to OpenShift Release and Upgrade Process for Cluster Administrators&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Updating to a newer z-stream release&lt;/h2&gt; &lt;p&gt;My test cluster is running release 4.5.4 and using the stable-4.5 channel. If I click &lt;strong&gt;Update now&lt;/strong&gt;, the web console presents me a list of releases that I can update to, with the more recent releases shown first (see Figure 3).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-3-update-to-4.5.17.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-3-update-to-4.5.17.png?itok=XFQi75B7" width="600" height="522" title="blog-3-update-to-4.5.17" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Selecting a version of OpenShift to update to after "Update now".&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Remember that the screenshot represents a point in time, in this case November 2020. You might see more (or fewer!) releases when you view them yourself.&lt;/p&gt; &lt;p&gt;The list might not include the latest release from your current channel. It includes only the ones with a direct update path from your current release. A few update cycles might take place before you get to the release you want or run out of update paths.&lt;/p&gt; &lt;p&gt;I select the 4.5.17 release, which is the latest as I write this article, and click &lt;strong&gt;Update&lt;/strong&gt;. After a few moments, the web console starts displaying the status of my update, as shown in Figure 4.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-4-working-to-4.5.17.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-4-working-to-4.5.17.png?itok=p7pJZDiv" width="600" height="225" title="blog-4-working-to-4.5.17" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: The update status.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The time to update depends on a number of factors, such as your internet bandwidth, size of the &lt;a href="https://developers.redhat.com/products/rhel "&gt;Red Hat Enterprise Linux&lt;/a&gt; CoreOS images and cluster operator images, number of nodes on your cluster, number of applications, their disruption budgets, and many other factors. My compact test cluster (only three nodes) took about 40 minutes.&lt;/p&gt; &lt;h2&gt;Web console quirks while updating&lt;/h2&gt; &lt;p&gt;During updates I saw a couple of times that the percent-complete indicator moved backward, for example from 70% to 25%. Don’t panic if that happens to you. I also saw a few temporary failures that disappeared without action from me, which I will explain later in this article. The upstream &lt;a href="https://github.com/openshift/cluster-version-operator/blob/master/docs/dev/upgrades.md#why-does-the-openshift-4-upgrade-process-restart-in-the-middle"&gt;Cluster Version Operator (CVO) documentation&lt;/a&gt; explains why that happens; it is by design.&lt;/p&gt; &lt;p&gt;At some time during the update, your web console session might expire because the update process restarts web console pods to use a new container image. You might even get a “server error” from the OpenShift router. If that happens, refresh your web browser, log in again (if needed), and navigate back to &lt;strong&gt;Administration—&gt;Cluster Settings&lt;/strong&gt; to continue monitoring the progress of your cluster update.&lt;/p&gt; &lt;p&gt;The stated OpenShift goal of 100% availability during updates does not mean that all HTTP requests to the web console and applications are successful. During an update, nodes are rebooted, pods are recreated, containers are stopped, and new containers are started. Load balancers might not react instantly and might send an occasional request to a container or a node that is not available. The next HTTP request should work.&lt;/p&gt; &lt;h2&gt;Is my OpenShift update done?&lt;/h2&gt; &lt;p&gt;When the update finishes, the web console shows your new current version and might also show that more updates are available (see Figure 5).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-5-version-4.5.17.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-5-version-4.5.17.png?itok=xb4Xv_FG" width="600" height="234" title="blog-5-version-4.5.17" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: The Cluster Settings screen after a successful update.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In my example, it looks like I still am unable to get to any 4.6 channel, but I see a single 4.5 release I could update to (see Figure 6).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-6-update-to-4.5.18.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-6-update-to-4.5.18.png?itok=dOdAdvfq" width="600" height="322" title="blog-6-update-to-4.5.18" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: Checking for newer versions on the Update Cluster screen after an update.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;So, I would repeat the process I've shown in this article to update to 4.5.18 and so on, in the hope of being able eventually to update to OpenShift 4.6. (I know that I do not need further updates, and I will explain later how I know that.)&lt;/p&gt; &lt;p&gt;Before you update again to the next z-stream release, refresh your browser tab. Your web browser might be displaying stale data from your previous cluster version. OpenShift 4.6 solves that particular issue and requires no browser refresh.&lt;/p&gt; &lt;h2&gt;Is my OpenShift update failing?&lt;/h2&gt; &lt;p&gt;I did a few cluster updates without any issues, from the same test environment. But one of my attempts displayed a scary, red exclamation icon and a “Failing” status, shown in Figure 7.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-7-failing.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-7-failing.png?itok=UHQ-cyEv" width="600" height="232" title="blog-7-failing" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: Status of "Failing".&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Calm down and notice that the status is “fail-&lt;em&gt;ing&lt;/em&gt;,” not “fail-&lt;em&gt;ed&lt;/em&gt;." The update process is still running. &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; clusters, applications, and operators can self-heal in a large number of scenarios. Sometimes the "Failing" message is caused by &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1884334"&gt;too short a timeout&lt;/a&gt; in the CVO. If that is the case, the message will fix itself with no action from you. And with no damage to your cluster and applications.&lt;/p&gt; &lt;p&gt;Do not panic, but give some time for OpenShift to heal itself. Be patient before you start collecting troubleshooting information and opening customer support tickets.&lt;/p&gt; &lt;h2&gt;Pending cluster operators&lt;/h2&gt; &lt;p&gt;If you click &lt;strong&gt;View details&lt;/strong&gt;, as shown in Figure 7, the web console shows you detailed information about the status of each cluster operator: Which ones have already finished updating and which ones are still performing their updates. Figure 8 shows that the &lt;code&gt;openshift-apiserver&lt;/code&gt; operator became degraded.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-8-degraded.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-8-degraded.png?itok=G3IkqUQE" width="600" height="337" title="blog-8-degraded" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 8: The cluster operators screen with a progress message showing a potential problem.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;I hope that the message “Cluster update &lt;em&gt;in progress&lt;/em&gt;," together with the less-alarming, blue information icon gives you some peace of mind: OpenShift is still performing the update; it has not failed at all.&lt;/p&gt; &lt;p&gt;Operators can become degraded for a while during updates and then recover by themselves. It is expected that, with each new OpenShift release, cluster operators become better at reporting their status and report fewer temporary failures.&lt;/p&gt; &lt;p&gt;Scrolling down, I can see that one of my API server pods is not running (see Figure 9). I know that this might happen when you update a Kubernetes deployment, so I decide to just give it some time to settle down.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-9-api-server.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-9-api-server.png?itok=f6zBNKwn" width="600" height="252" title="blog-9-api-server" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 9: A degraded pod on the Cluster Operators screen.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If I suspected that something was wrong, maybe because I saw other operators with errors, I would check the operator logs and cluster events to find a hint of a real unrecoverable error.&lt;/p&gt; &lt;p&gt;After a few minutes, the “Failing” state switches to “Update available” and the desired cluster version, 4.5.17, becomes the current version. My update finishes successfully, as shown in Figure 10.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-10-update-avail.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-10-update-avail.png?itok=5jpzTpRz" width="600" height="234" title="blog-10-update-avail" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 10: The Cluster Settings screen showing the current version after an update.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;That “Failing” state could also switch to a new percent-complete message, as if the update process is just progressing and no failure ever happened. You might even see the installation go into a “Failing” state and then back to “Working towards” states a few times.&lt;/p&gt; &lt;p&gt;In the end, the changes you notice depend on how much attention you pay to the web console while your cluster is updating. Because I know the update is supposed to take time, I usually perform other tasks instead of staring at the OpenShift web console’s &lt;strong&gt;Cluster Settings&lt;/strong&gt; page. If something really bad happens during my update, I expect OpenShift cluster monitoring to alert me.&lt;/p&gt; &lt;h2&gt;Updating to a newer minor release&lt;/h2&gt; &lt;p&gt;You might not be able to update from your current 4.y.z release to any 4.y+1 release, using the web console, prior to OpenShift 4.6. It might be necessary to update to a newer 4.y.z+d before the web console shows a channel for the next minor release. You might even have to perform multiple z-stream updates.&lt;/p&gt; &lt;p&gt;In my sample scenario, I had to update from OpenShift 4.5.3 to 4.5.17 to get the possibility of switching to the stable-4.6 channel. With my cluster updated to 4.5.17, and also after a web browser page refresh, the web console offered me the choice of a stable channel for the next minor release of OpenShift (see Figure 11).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-11-update-4.6.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-11-update-4.6.png?itok=QkLUUgC6" width="600" height="452" title="blog-11-update-4.6" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 11: The Update Channel screen showing 4.6 releases.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To update to the next minor release, click on &lt;strong&gt;stable-4.6&lt;/strong&gt; to switch channels, click &lt;strong&gt;Update now&lt;/strong&gt;, and then select any of the available 4.6.z releases as the new OpenShift release for your cluster. The update process from now on is the same as a z-stream update.&lt;/p&gt; &lt;h2&gt;Is it too soon to update?&lt;/h2&gt; &lt;p&gt;The first time I tried to update from OpenShift 4.5 to 4.6, by switching from the stable-4.5 to the stable-4.6 channel, I got the disappointing “Version not found” status in Figure 12. It didn't make any difference if I didn't stop at 4.5.17, but continued to update to 4.5.18 or later.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-12-update-not-found.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-12-update-not-found.png?itok=28XM7I8T" width="600" height="204" title="blog-12-update-not-found" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 12: The update was not found.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The message means that the stable-4.6 channel does not include my current version. At that time, the stable-4.6 channel did not include 4.5.18 (nor 4.5.17, for that matter).&lt;/p&gt; &lt;p&gt;Figure 13 shows a fully updated OpenShift 4.5 cluster, according to its current release update channel (stable-4.5), so you can compare it to the previous figure. The “Up to date” status means that the channel lists my current release only as an update path destination, and not as a source of any path. So there are no updates available currently, but an update might become available in the future.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-13-up-to-date.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-13-up-to-date.png?itok=xdroVXi4" width="600" height="252" title="blog-13-up-to-date" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 13: The Cluster Settings screen when the OpenShift version is up to date.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If you find yourself in a similar scenario (that is, you switch to the stable channel of the next minor release, and find that no updates are available), Red Hat recommends that you switch back to a channel that lists your current release so you do not miss bug fixes and security updates. In my example, I would switch back to stable-4.5. Indeed, a few days later that channel got 4.5.19 and 4.5.20. But none of these releases are published to stable-4.6 as I write.&lt;/p&gt; &lt;p&gt;Because I tried checking only a few days after the first generally available (GA) release of OpenShift 4.6, all update paths were likely waiting for proof of stability from clusters using the fast channel.&lt;/p&gt; &lt;p&gt;If you find yourself in the same situation, you have two choices: Either check the stable-4.6 channel again from time to time, until you see a 4.6.z release update on the stable-4.6 channel, and revert back to the stable-4.5 after each check; or switch to the fast channel in the hope that there is an update path there.&lt;/p&gt; &lt;h2&gt;Using the fast channel&lt;/h2&gt; &lt;p&gt;In my case, after I switch to the fast-4.6 channel, I see that there are indeed updates available (see Figure 14).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-13-up-to-date.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-13-up-to-date.png?itok=xdroVXi4" width="600" height="252" title="blog-13-up-to-date" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 14: The Cluster Settings screen for the fast-4.6 channel shows updates available.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;And I could pick two releases to update from 4.5.18: 4.6.3 (not shown in Figure 15) and 4.6.4.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-15-update-to-4.6.4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-15-update-to-4.6.4.png?itok=UaBYqOvp" width="600" height="324" title="blog-15-update-to-4.6.4" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 15: Version 4.6.4 selected for update.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Figure 16 is proof that my test cluster finished its update to 4.6.4 using the fast channel. Notice the change in the look and feel of the web console. After OpenShift 4.7 is released, I will write a new article with new screenshots.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-16-version-4.6.4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-16-version-4.6.4.png?itok=o0wa5AO_" width="600" height="270" title="blog-16-version-4.6.4" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 16: The Cluster Settings screen after version 4.6.4 is selected for update.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Do not be afraid of updating your cluster using a fast channel if you need an update right now. Red Hat fully supports using all releases from the fast channel in production environments. Make the decision to switch, or wait based on your needs for these updates and the potential impacts of waiting a little longer to update.&lt;/p&gt; &lt;h2&gt;How to find which updates are available?&lt;/h2&gt; &lt;p&gt;You might not consider it intuitive that the OpenShift web console allows you to switch to a channel when no updates are available. It makes sense once you consider that you select a channel to signal your intent of getting updates from that channel. If you pick a very new channel, as I did, there might be very few update paths. Then it is no surprise that a given release is not in any of them.&lt;/p&gt; &lt;p&gt;Having to switch channels and wait for intermediate cluster updates before you find whether you can update to the desired OpenShift release could be frustrating. Fortunately, there are multiple ways of checking for available updates. OpenShift 4.6 allows you to find them before performing an update. But because I am still on 4.5, I have to search outside of the web console.&lt;/p&gt; &lt;p&gt;One way is to use the &lt;a href="https://ctron.github.io/openshift-update-graph/#stable-4.6"&gt;OpenShift Update Graph&lt;/a&gt;. It shows a nice-looking, though sometimes confusing, graph of all update paths available for the selected channel. For example, I got the results shown in Figure 17 in mid-November 2020.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-17-graph-stable.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-17-graph-stable.png?itok=_WWAX_iS" width="222" height="226" title="blog-17-graph-stable" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 17: Limited versions were available in the OpenShift Update Graph.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;At the time I generated Figure 17, there was no update path from any 4.5.z release to 4.6 in the stable channel. When I instead selected the fast-4.6 channel, I got the much richer graph in Figure 18.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-18-graph-fast.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-18-graph-fast.png?itok=cYwNojjy" width="600" height="490" title="blog-18-graph-fast" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 18: Numerous versions shown in the OpenShift Update Graph.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To help you visualize the state of that graph at the time I captured the screenshots for this article, I colored all 4.5.z releases with arrows getting to any 4.6.z release and zoomed in so that this part of the graph is easier to read (see Figure 19).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/blog-19-zoom-fast.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/blog-19-zoom-fast.png?itok=0z6VnSce" width="326" height="317" title="blog-19-zoom-fast" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 19: Focusing on 4.5.z versions in the OpenShift Update Graph.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Figure 19 makes it easier to see that I could update from 4.5.16, 4.5.17, 4.5.18, and 4.5.19 to an OpenShift 4.6 release using the fast-4.6 channel.&lt;/p&gt; &lt;h2&gt;Finding available updates from the shell&lt;/h2&gt; &lt;p&gt;Another way to list update paths from an OpenShift 4 update channel comes from &lt;a href="https://access.redhat.com/solutions/4583231"&gt;this article&lt;/a&gt;. The following example lists all updates available for 4.5.4 from the stable-4.5 channel. The last command includes a very long &lt;code&gt;jq&lt;/code&gt; filter. Make sure the entire filter is a single shell argument. Notice the single quotes around it.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export CURRENT_VERSION=4.5.4 $ export CHANNEL_NAME=stable-4.5 $ curl -sH 'Accept:application/json' "https://api.openshift.com/api/upgrades_info/v1/graph?channel=${CHANNEL_NAME}" | jq -r --arg CURRENT_VERSION "${CURRENT_VERSION}" '. as $graph | $graph.nodes | map(.version=='\"$CURRENT_VERSION\"') | index(true) as $orig | $graph.edges | map(select(.[0] == $orig)[1]) | map($graph.nodes[.].version) | sort_by(.)' [ "4.5.11", "4.5.13", "4.5.14", "4.5.15", "4.5.16", "4.5.17", "4.5.5", "4.5.6", "4.5.7", "4.5.8", "4.5.9", ]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;All those pipe signs (|) belong to the &lt;code&gt;jq&lt;/code&gt; filter. They are part of the fifth argument of the &lt;code&gt;jq&lt;/code&gt; command. Do not mistake them for shell pipes! Also note that the &lt;code&gt;jq&lt;/code&gt; filter sorts its output in a somewhat misleading way. It uses string ordering, not semantic version ordering. Notice that the most recent version, which was 4.5.17, appears in the middle of the output.&lt;/p&gt; &lt;p&gt;An alternative to writing that long &lt;code&gt;jq&lt;/code&gt; filter is using the &lt;a href="https://github.com/openshift/cincinnati/blob/master/hack/available-updates.sh"&gt;available-updates.sh&lt;/a&gt; script from the Cincinnati developers. The CVO uses the Cincinnati protocol to describe update channels.&lt;/p&gt; &lt;p&gt;Download the &lt;code&gt;available-updates.sh&lt;/code&gt; script and make it executable. Then set the &lt;code&gt;CHANNEL&lt;/code&gt; environment variable and pass the starting version as an argument. The following example lists what would be my options after I update to 4.5.17, using the fast-4.6 channel:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export CHANNEL=fast-4.6 $ ./available-updates.sh 4.5.17 4.5.18 quay.io/openshift-release-dev/ocp-release@sha256:72e3..f366 https://access.redhat.com/errata/RHBA-2020:4425 4.6.4 quay.io/openshift-release-dev/ocp-release@sha256:6681..86fc https://access.redhat.com/errata/RHBA-2020:4987&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If I try the stable-4.6 channel I get an empty list, that is, &lt;code&gt;[]&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;By viewing the output of these commands, I was able to plan for two updates starting from 4.5.4: first to 4.4.17 and then to 4.6.4 using the fast channel.&lt;/p&gt; &lt;p&gt;As you might guess, many developers created their own visualization tools and scripts to report OpenShift cluster updates. An example is &lt;a data-pjax="#js-repo-pjax-container" href="https://github.com/pamoedom/ocp4upc"&gt;ocp4upc&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;OpenShift 4 updates do not have to be scary. The web console makes updates easy to do, and the underlying infrastructure provided by the CVO makes the process very reliable and dependable.&lt;/p&gt; &lt;p&gt;There are a few quirks in the way the web console handles updates up to OpenShift 4.5. Fortunately, OpenShift 4.6 solves most of them.&lt;/p&gt; &lt;p&gt;The OpenShift update process is very consistent with Kubernetes’s design patterns: You declare the desired state of your cluster and let Kubernetes converge the current state to the desired state.&lt;/p&gt; &lt;p&gt;For more information about how the OpenShift CVO and cluster operators handle cluster updates, please see the video &lt;a href="https://www.openshift.com/blog/red-hat-openshift-cluster-upgrades-and-application-operator-updates"&gt;Red Hat OpenShift: Cluster Upgrades and Application Operator Updates&lt;/a&gt; and also the excellent post from the official OpenShift blog: &lt;a href="https://www.openshift.com/blog/the-ultimate-guide-to-openshift-release-and-upgrade-process-for-cluster-administrators"&gt;The Ultimate Guide to OpenShift Release and Upgrade Process for Cluster Administrators&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Acknowledgments&lt;/h2&gt; &lt;p&gt;Thanks to Eric Rich and Mike Allmen for their reviews of drafts of this article. Also thanks to W. Trevor King and Scott Dodson for their many valuable comments improving the technical information in this article. If you find any errors and inaccuracies in this article, they are mine only despite their best efforts.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/05/27/how-update-newer-red-hat-openshift-4-releases" title="How to update to newer Red Hat OpenShift 4 releases"&gt;How to update to newer Red Hat OpenShift 4 releases&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Q17YrPMCT1I" height="1" width="1" alt=""/&gt;</summary><dc:creator>Fernando Lozano</dc:creator><dc:date>2021-05-27T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/05/27/how-update-newer-red-hat-openshift-4-releases</feedburner:origLink></entry><entry><title type="html">Writing fast constraints with OptaPlanner: the secret recipe</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/L3HEB14oenY/writing-fast-constraints-with-optaplanner-the-secret-recipe.html" /><author><name>triceo</name></author><id>http://feeds.athico.com/~r/droolsatom/~3/eaW922tbOWw/writing-fast-constraints-with-optaplanner-the-secret-recipe.html</id><updated>2021-05-27T00:00:00Z</updated><content type="html">Do you want OptaPlanner to run faster? Do you want to increase your score calculation speed, reaching great solutions sooner? Let me show you how to optimize your constraints for performance and scalability. Turns out you only need to remember one advice: DO LESS The key to well-performing constraints is limiting the amount of data that flows through your Constraint Streams, which starts with . Consider a school timetabling problem, where a teacher must not have two overlapping lessons. This is how the lesson could look in Java: @PlanningEntity class Lesson { ... Teacher getTeacher() { ... } boolean overlaps(Lesson anotherLesson) { ... } boolean isCancelled() { ... } ... } The simplest possible Constraint Stream we could write to penalize all overlapping lessons would then look like: constraintFactory.from(Lesson.class) .join(Lesson.class) .filter((leftLesson, rightLesson) -&gt; !leftLesson.isCancelled() &amp;amp;&amp; !rightLesson.isCancelled() &amp;amp;&amp; leftLesson.getTeacher() .equals(rightLesson.getTeacher()) &amp;amp;&amp; leftLesson.overlaps(rightLesson)) .penalize("Teacher lesson overlap", HardSoftScore.ONE_HARD) What this Constraint Stream does is: 1. It creates all possible pairs of Lessons from the planning solution. 2. Then it all the lessons that are cancelled, where the teachers do not match, or which do not overlap. 3. It all the remaining lesson pairs. Do you see the problem here? The join creates a cross product between lessons, producing a match (also called a tuple) for every possible combination of two lessons, even though we know that many of these matches will not be penalized. This shows the problem in numbers: In order to process a thousand lessons, our constraint first creates a cross product of 1 million pairs, only to throw away pretty much all of them before penalizing! If we can reduce the size of the cross product by half, only half of the time will be spent processing it. This is where the original advice comes into play: do less, by avoiding unrestricted cross product. Here’s how. Table 1. Fast growth of cross product Number of lessons Number of possible pairs 10 100 100 10 000 1 000 1 000 000 FILTER BEFORE JOINING As you can see from the first example, cancelled lessons are eventually filtered out after the join. Let’s see if we can remove them from the cross product instead. For the first lesson in the join (also called “left”), this is straightforward; we simply bring the cancellation check before the join like so: constraintFactory.from(Lesson.class) .filter(lesson -&gt; !lesson.isCancelled()) .join(Lesson.class) .filter((leftLesson, rightLesson) -&gt; !rightLesson.isCancelled() &amp;amp;&amp; leftLesson.getTeacher() == rightLesson.getTeacher() &amp;amp;&amp; leftLesson.overlaps(rightLesson)) .penalize("Teacher lesson overlap", HardSoftScore.ONE_HARD) The cancelled lessons are no longer coming in from the left, which reduces the cross product. However, some cancelled lessons are still coming in from the right through the join. Here, we will use a little trick and join not with a Lesson class, but with a filtered nested Constraint Stream instead: constraintFactory.from(Lesson.class) .filter(lesson -&gt; !lesson.isCancelled()) .join( constraintFactory.from(Lesson.class) .filter(lesson -&gt; !lesson.isCancelled())) .filter((leftLesson, rightLesson) -&gt; leftLesson.getTeacher() == rightLesson.getTeacher() &amp;amp;&amp; leftLesson.overlaps(rightLesson)) .penalize("Teacher lesson overlap", HardSoftScore.ONE_HARD) As you can see, we’ve created a new Constraint Stream from Lesson, filtering before it entered our join. We have now applied the same improvement on both the left and right sides of the join, making sure it only creates a cross product of lessons which we care about. But we can still do better! PREFER JOINERS TO FILTERS Filters are just a simple check if a tuple matches a predicate. If it does, it is sent downstream, otherwise the tuple is removed from the Constraint Stream. Each tuple needs to go through this check, and that means every pair of lessons will be evaluated. When a Lesson changes, all pairs with that Lesson will be re-evaluated, but not anymore: constraintFactory.from(Lesson.class) .filter(lesson -&gt; !lesson.isCancelled()) .join( constraintFactory.from(Lesson.class) .filter(lesson -&gt; !lesson.isCancelled()), Joiners.equal(Lesson::getTeacher)) .filter((leftLesson, rightLesson) -&gt; leftLesson.overlaps(rightLesson)) .penalize("Teacher lesson overlap", HardSoftScore.ONE_HARD) Notice that the Teacher equality check moved from the final filter to something called a Joiner. We are still saying the same thing – a Lesson pair will only be sent downstream if the Lessons share the same Teacher. Unlike the filter, this brings the performance benefit of indexing. Now when a Lesson changes, only the pairs with the matching Teacher will be re-evaluated. So even though the cross-product remains the same, we are doing much less work processing it. The final filter now only performs one operation on the final cross product, and the Lesson pairs that get this far are already trimmed down in the most efficient way possible. REMOVE MORE, EARLIER In some cases, you may have an option to pick the order of your Joiners. In these situations, you should put first the Joiner that will remove more tuples than the others. This will reduce the size of your cross products faster. Consider a new situation, where lessons also have rooms in which they happen. Although there are possibly dozens of teachers, there are only three rooms. Therefore the join should look like this: constraintFactory.from(Lesson.class) .join(Lesson.class, Joiners.equal(Lesson::getTeacher), Joiners.equal(Lesson::getRoom)) ... This way, we first create “buckets” for each of the many teachers, and these buckets will only contain a relatively small number of lessons per room. If we did it the other way around, there would be a small amount of large buckets, leading to much more iteration every time a lesson changes. For that reason, it is generally recommended putting Joiners based on enum fields or boolean fields last. CONCLUSION The key to efficient constraints is the reduction of cross product. There are three main ways of reducing cross product in Constraint Streams: 1. Filtering before joining. 2. Preferring Joiners earlier to filtering later. 3. Applying the more restrictive Joiners first. There are other optimization techniques as well, and we will discuss some of them in the future, but none of them will give as big a benefit as reducing the size of cross products. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/L3HEB14oenY" height="1" width="1" alt=""/&gt;</content><dc:creator>triceo</dc:creator><feedburner:origLink>http://feeds.athico.com/~r/droolsatom/~3/eaW922tbOWw/writing-fast-constraints-with-optaplanner-the-secret-recipe.html</feedburner:origLink></entry><entry><title type="html">Tooling guide for Getting Started with Apache Camel in 2021</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/CugRKVT82PY/tooling-guide-for-getting-started-with.html" /><author><name>CHRISTINA の J老闆</name></author><id>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/N9sx2zTVps4/tooling-guide-for-getting-started-with.html</id><updated>2021-05-26T13:20:00Z</updated><content type="html">Getting Started with Apache Camel ? This post is for you. But I am not going to dive into how to write the Camel route, there are plenty of materials out there that do a better job than me. A good place to get started is the one and only , it gives you all the latest and greatest news from the community. If you want to start from the basics, I highly recommend Camel in Action II Book. It has everything you need to know about writing Camel. Or join the Camel community or chat  to ask questions, it's very friendly and welcoming. If you are a returning Camel rider, I would go through this , where it will get you up to speed on what to expect.  Another good resource is the page. I found the majority of the getting started enquiries can be found here.  This post will be about tools that can help you, when it comes to running a Camel application in its lifetime. From design, implementation, testing, deployment and monitoring. Of course this is strictly my opinion. Any suggestions or comments are welcome. Here is a quick collection of the tools I recommend.  DESIGN &amp;amp; IMPLEMENTATION There are several different IDEs out there that have language support for Camel. My current goto is the . VS Code itself has very primitive support for Java developers, but it comes with a large variety of extensions for you to customize it, and install all the tools you need. Speaking of extension, there is a must have… “”. It contains the essentials for developing Camel. I recommend checking out the for updates!  To begin a project, it’s always good to have some help to set the basic structure. You can either use the Project Initializer that comes with the Apache Camel by Red Hat Extension in VS Code that will generate the base project for you. Or you can always go to the Quarkus start coding page, adding the Camel dependencies to create the base project, but one downside of the Quarkus start coding page, it will not create the template extended RouteBuilder class for you.  No matter what runtime you choose, you are still writing Camel DSL (Domain Specific Language), you have the latest and greatest support from the camel extension, where it will help you with autocomplete, validating your Camel code.   Mapping data between formats can be tedious, especially with larger documents or more complex structure. But using the tooling can help you map between two data sources with a drag and drop user interface, so you will be able to visualize the mappings. The tool will generate an “.adm” file, place the file into your Camel project. You can then use the camel-atlasmap components to run the mapping.   from("servicenow:xxxxxx....")   .split().body()     .marshal().json()     .to("atlasmap:servicenow.adm")     .to("kafka:xxx?brokersxxx") RESTful API is another popular implementation in Camel, I highly recommend you take advantage of the Apicurio project, where it provides a GUI interface for designing the API contract. Making contract first application development a lot easier. For more details, take a look at my previous .  Another nice complements to the toolset is the , this is another extension in VS Code by Bruno, this can help you visualize the design of camel processing flow. So you will be able to see what your camel route does in a quick glance. TESTING  Camel can easily wire up unit tests in your preferred testing framework. My past experience has been great with using JUnit for creating my test cases.   What I normally would do is create a java Class for each test case (Extend “CamelTestSupport” class). Where it can kick off your camel route with mock endpoints, loading set of data or even stub out the actual endpoint When testing integration, introducing Behavior-Driven Development(BDD) Testing is a development practice, as it’s black box testing nature allows it to better mimic user input and expectation, and can test more complex scenarios when compared to Test-Driven Development(TDD). Consider using the “” for building out BDD test cases. You can define the test scenario descriptions with a "Given-When-Then" structure in a feature file, as well as a simple setup for mimicking the common endpoints such as Kafka, AMQP, JMS, REST… etc.   Feature: integration runs   Background:     Given load variables namespace.properties     Given URL: http://camel-hello-quarkus.${namespace}.svc.cluster.local:8080   Scenario: Checking GET Method     When send GET /prescription?profileid=123456     Then verify HTTP response body: {"profileid": 123456, "pharmancy": "CVS" }     Then receive HTTP 200 OK I have also been using a spinoff project from Citrus called YAKs to do my BDD testing on Kubernetes(OpenShift) platform. And I LOVED it. Simply because 1st, the lifecycle of the test was managed by the YAKs operator, meaning the test was run on a separate instance, that is where the client supposed the call, and also the YAKs operator takes in the feature, and does the test based on it. You can run it separately or plug it into your CI/CD pipeline. PLATFORM I have been using for managing all the running instances. It's just easier when there are hundreds of Camels running, it manages the scaling, resources, configuration and load-balancing for me. As well as a better deployment module (images). There are so many articles out there talking about the benefits of running a container &amp;amp; container management, so I won’t dive into it.  CI/CD Jenkin was on my list for a long time when it comes to building the pipeline for my Camel application. Basically using the maven jenkins plugin for compiling and deploying, and using the OpenShift plugin to do the canary releases. And recently, I have been using the Tekton pipeline, it’s built-in on OpenShift, so there is no need to install a separate instance and manage it. The Kubernetes platform itself will become the CI/CD platform itself. Here is an example of how it works with the Camel projects. I will write another deep dive into how it works. MONITORING With Camel 3, in order to collect and export the metrics, you will need to specifically add the microprofile-metrics dependency. And by using Prometheus to scrape metrics on the platform. It stores all scraped samples locally and aggregate the time series data. I then use Grafana to visualize the collected data, and create a dashboard to monitor all the camel routes. Also checkout this video and see how things work:&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/CugRKVT82PY" height="1" width="1" alt=""/&gt;</content><dc:creator>CHRISTINA の J老闆</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/N9sx2zTVps4/tooling-guide-for-getting-started-with.html</feedburner:origLink></entry></feed>
